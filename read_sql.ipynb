{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('connector3.8': venv)",
   "metadata": {
    "interpreter": {
     "hash": "ddac47d27da2a62cca88fe099e8765341cd7cac6e49ea98650ef5a79d0c7483b"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# read_sql(): read data from database to pandas/dask dataframe through a sql query\n",
    "\n",
    "## Introduction\n",
    "Databases is one of the most commonly used data source that data scientists fetch data from. However, the transformation process to load data from database and convert it into dataframes for further analyze is usually heavy-weight. The `read_sql` function aims to speed up the process through the following features:\n",
    "\n",
    "* Query partition: split a big query into a bunch of small queries so we can make the procedures like query execution, data transfer and format conversion in parallel and merge the results of small queries in the end\n",
    "* Result cache: offer persistence of the fetched data, do not need to repeatedly download data in situations like notebook restart or applying different tasks on the same dataset\n",
    "* Fast data conversion: speed up the csv to pandas process using 1. parallelism and 2. directly write to pre-allocated pandas dataframe memory buffer\n",
    "\n",
    "## User API\n",
    "```python\n",
    "read_sql(sql, conn, cache=True, force_download=False, par_column=None, par_min=None, par_max=None, par_num=None, dask=False)\n",
    "```\n",
    "\n",
    "### Parameters\n",
    "* **sql**(string) - The sql query for fetching the data\n",
    "* **conn**(string) - Connection string uri (e.g. `postgresql://username:password@host:port/dbname`)\n",
    "* **cache**(string or bool, optional(default `True`)) - Whether or not to cache the result data. If `False` is set, do not cache the result. If `True` is set, cache the result to `/tmp` with connection and sql as name. If a string is set, cache the result to the corresponding path\n",
    "* **force_download**(bool, optional(default `False`)) - Whether or not to force download the data from database no matter there is a cache or not\n",
    "* **par_column**(string, optional(default `None`)) - Name of column used to partition the query (Must be a integer column). If `None` is set, do not do partition\n",
    "* **par_min**(int, optional(default `None`)) - The minimum value to be requested from the partition column `col`. If `None` is set, do not do partition\n",
    "* **par_max**(int, optional(default `None`)) - The maximum value to be requested from the partition column `col`. If `None` is set, do not do partition\n",
    "* **par_num**(int, optional(default `None`)) - Number of queries to split. If `None` is set, do not do partition\n",
    "* **dask**(bool, optional(default False)) - Whether to return Dask dataframe instead of Pandas dataframe\n",
    "\n",
    "### Result\n",
    "Pandas/Dask DataFrame\n",
    "\n",
    "## Related Works\n",
    "* Query partition\n",
    "    * [PySpark JDBC DataFrameReader](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.jdbc)\n",
    "    * [Modin Parallel read_sql 1](https://github.com/modin-project/modin/pull/499)\n",
    "    * [Modin Parallel read_sql 2 (experimental)](https://github.com/modin-project/modin/pull/436/commits)\n",
    "* Result cache\n",
    "    * [Oracle Client Result Cache](https://www.appservgrid.com/documentation111/docs/rdbms12cr1/ADFNS/adfns_perf_scale.htm#ADFNS213)\n",
    "    * [Incremental caching support in ComponentOne](https://www.grapecity.com/componentone/docs/services/online-dataconnector/incrementalcaching.html)\n",
    "    * [Opening issue for cache in pandas.read_sql](https://github.com/pandas-dev/pandas/issues/22208)\n",
    "    "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Plan\n",
    "\n",
    "* Target use case: Fetch data from PostgreSQL to pandas dataframe\n",
    "* Tasks (expect save time calculated on TPCH scale=10, lineitem table (60M rows), 10 workers, 158s in total):\n",
    "    * Implement parallel read_csv in Rust arrow - contribute code to arrow (expect save time 12% [158s->138s])\n",
    "    * Read directly into pandas memory from DB, do not need to convert arrow to pandas (expect save time 52% [158s->75s])\n",
    "    * Implement a cache on the client side for reloading the same data\n",
    "        * Finish the functionality, do not consider incremental update\n",
    "        * Research on how to incremental update\n",
    "    * Partition the query and connect to DB in parallel (naive partition to 10 queries compared with 1 query saves time 67% [490s -> 158s])\n",
    "        * Finish the functionality, do not consider how to partition\n",
    "        * Research on how to do the partition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}